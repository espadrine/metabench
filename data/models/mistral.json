{
  "models": [
    {
      "name": "Devstral 2 123B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/devstral-2-vibe-cli",
      "release_date": "2025-12-09",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.4,
          "source": "https://mistral.ai/news/devstral-2-vibe-cli"
        },
        {
          "name": "Output cost",
          "score": 2,
          "source": "https://mistral.ai/news/devstral-2-vibe-cli"
        },
        {
          "name": "Size",
          "score": 125.02598984,
          "source": "https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512"
        },
        {
          "name": "Active parameters",
          "score": 125.02598984,
          "source": "https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 5.4,
          "source": "Guessed to be equal to Devstral Small 1.1 since no data available"
        },
        {
          "name": "SWE-bench Verified",
          "score": 72.2,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 61.3,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 32.6,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 21.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 23.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 36.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 76.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 59.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 44.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 33.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 36.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 38.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 30,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 18.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 24.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Devstral Small 2 24B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/devstral-2-vibe-cli",
      "release_date": "2025-12-09",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://mistral.ai/news/devstral-2-vibe-cli"
        },
        {
          "name": "Output cost",
          "score": 0.3,
          "source": "https://mistral.ai/news/devstral-2-vibe-cli"
        },
        {
          "name": "Size",
          "score": 24.01136184,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "Active parameters",
          "score": 24.01136184,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 5.4,
          "source": "Guessed to be equal to Devstral Small 1.1 since no data available"
        },
        {
          "name": "SWE-bench Verified",
          "score": 68,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 55.7,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 22.5,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 19.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 20.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 34.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 67.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 53.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 34.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 28.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 34.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 31.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 24,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 16.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 23.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mistral Large 3",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-3",
      "release_date": "2025-12-02",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 1.5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 675,
          "source": "https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512"
        },
        {
          "name": "Active parameters",
          "score": 41,
          "source": "https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 11,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "MMMLU",
          "score": 85.5,
          "source": "https://mistral.ai/news/mistral-3"
        },
        {
          "name": "GPQA Diamond",
          "score": 43.9,
          "source": "https://mistral.ai/news/mistral-3"
        },
        {
          "name": "SimpleQA",
          "score": 23.8,
          "source": "https://mistral.ai/news/mistral-3"
        },
        {
          "name": "AMC",
          "score": 52,
          "source": "https://mistral.ai/news/mistral-3"
        },
        {
          "name": "LiveCodeBench",
          "score": 34.4,
          "source": "https://mistral.ai/news/mistral-3"
        },
        {
          "name": "LMArena Text",
          "score": 1411,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 22.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 22.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 38,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 80.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 68,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 46.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 36.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 38,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 36.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 34.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 15.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 24.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Ministral 3 14B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-3",
      "release_date": "2025-12-02",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 13.94503168,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "Active parameters",
          "score": 13.94503168,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "AIME 2025",
          "score": 85,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "GPQA Diamond",
          "score": 71.2,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "LiveCodeBench",
          "score": 64.6,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "HMMT 2025",
          "score": 67.5,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        }
      ]
    },
    {
      "name": "Ministral 3 14B Instruct",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-3",
      "release_date": "2025-12-02",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 13.94503168,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "Active parameters",
          "score": 13.94503168,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 19,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "Arena-Hard",
          "score": 55.1,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "WildBench",
          "score": 68.5,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "MATH",
          "score": 90.4,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "MM MT-Bench",
          "score": 8.49,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 16.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 10.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 30,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 69.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 57.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 35.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 23.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 30,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 32,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 22,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 4.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 27.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Ministral 3 8B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-3",
      "release_date": "2025-12-02",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.15,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.15,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 8.91802624,
          "source": "https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512"
        },
        {
          "name": "Active parameters",
          "score": 8.91802624,
          "source": "https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512"
        },
        {
          "name": "AIME 2025",
          "score": 78.7,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "GPQA Diamond",
          "score": 66.8,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "LiveCodeBench",
          "score": 61.6,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        }
      ]
    },
    {
      "name": "Ministral 3 8B Instruct",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-3",
      "release_date": "2025-12-02",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.15,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.15,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 8.91802624,
          "source": "https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512"
        },
        {
          "name": "Active parameters",
          "score": 8.91802624,
          "source": "https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 21,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "Arena-Hard",
          "score": 50.9,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "WildBench",
          "score": 66.8,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "MATH",
          "score": 87.6,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "MM MT-Bench",
          "score": 8.08,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 15.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 10,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 31.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 64.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 47.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 30.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 20.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 31.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 29.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 24,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 4.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 26.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Ministral 3 3B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-3",
      "release_date": "2025-12-02",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.1,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 4.251743232,
          "source": "https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512"
        },
        {
          "name": "Active parameters",
          "score": 4.251743232,
          "source": "https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512"
        },
        {
          "name": "AIME 2025",
          "score": 72.1,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "GPQA Diamond",
          "score": 53.4,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "LiveCodeBench",
          "score": 54.8,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        }
      ]
    },
    {
      "name": "Ministral 3 3B Instruct",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-3",
      "release_date": "2025-12-02",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.1,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 4.251743232,
          "source": "https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512"
        },
        {
          "name": "Active parameters",
          "score": 4.251743232,
          "source": "https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 6.8,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "Arena-Hard",
          "score": 30.5,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "WildBench",
          "score": 56.8,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "MATH",
          "score": 83,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "MM MT-Bench",
          "score": 7.83,
          "source": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 12.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 4.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 22,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 52.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 35.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 5.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 24.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 14.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 22,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 26.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 11.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 24.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Magistral Medium 1.2",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/magistral",
      "release_date": "2025-09-24",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 77,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "AIME 2024",
          "score": 91.82,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        {
          "name": "AIME 2025",
          "score": 83.48,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        {
          "name": "GPQA Diamond",
          "score": 76.26,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        {
          "name": "LiveCodeBench",
          "score": 75,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        {
          "name": "GPQA",
          "score": 70.8,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 27.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 21.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 82,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 81.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 73.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 9.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 75,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 39.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 82,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 43,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 51.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 12.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 52,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Magistral Small 1.2",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/magistral",
      "release_date": "2025-09-24",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 1.5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 24.01136128,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        {
          "name": "Active parameters",
          "score": 24.01136128,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 62,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "AIME 2024",
          "score": 86.14,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        {
          "name": "AIME 2025",
          "score": 77.34,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        {
          "name": "GPQA Diamond",
          "score": 70.07,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        {
          "name": "LiveCodeBench",
          "score": 70.88,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 22.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 14.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 80.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 76.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 66.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 6.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 72.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 35.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 80.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 44.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 16.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 4.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 27.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mistral Medium 3.1",
      "company": "Mistral AI",
      "url": "https://x.com/MistralAI/status/1959015454359585230",
      "release_date": "2025-08-23",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.4,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 12,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "LMArena Text",
          "score": 1436,
          "source": "https://x.com/MistralAI/status/1959015454359585230"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 21.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 18.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 38.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 68.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 58.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 40.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 33.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 38.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 39.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 19.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 10.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 40.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Magistral Medium 1.1",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/magistral",
      "release_date": "2025-07-24",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "AIME 2024",
          "score": 72.03,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2507"
        },
        {
          "name": "AIME 2025",
          "score": 60.99,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2507"
        },
        {
          "name": "GPQA Diamond",
          "score": 71.46,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2507"
        },
        {
          "name": "LiveCodeBench",
          "score": 59.35,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2507"
        }
      ]
    },
    {
      "name": "Magistral Small 1.1",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/magistral",
      "release_date": "2025-07-24",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 1.5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 23.5724032,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2507"
        },
        {
          "name": "Active parameters",
          "score": 23.5724032,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2507"
        },
        {
          "name": "AIME 2024",
          "score": 70.52,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2507"
        },
        {
          "name": "AIME 2025",
          "score": 62.03,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2507"
        },
        {
          "name": "GPQA Diamond",
          "score": 65.78,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2507"
        },
        {
          "name": "LiveCodeBench",
          "score": 59.17,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2507"
        }
      ]
    },
    {
      "name": "Magistral Medium 1.0",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/magistral",
      "release_date": "2025-06-10",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 150,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "AIME 2024",
          "score": 73.59,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2506"
        },
        {
          "name": "AIME 2025",
          "score": 64.95,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2506"
        },
        {
          "name": "GPQA Diamond",
          "score": 70.83,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2506"
        },
        {
          "name": "LiveCodeBench",
          "score": 59.36,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2506"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 19.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 16,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 40.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 75.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 67.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 9.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 52.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 29.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 91.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 70,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 40.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 25.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 9.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 23.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Magistral Small 1.0",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/magistral",
      "release_date": "2025-06-10",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 1.5,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 23.5724032,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2506"
        },
        {
          "name": "Active parameters",
          "score": 23.5724032,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2506"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 140,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "AIME 2024",
          "score": 70.68,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2506"
        },
        {
          "name": "AIME 2025",
          "score": 62.76,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2506"
        },
        {
          "name": "GPQA Diamond",
          "score": 68.18,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2506"
        },
        {
          "name": "LiveCodeBench",
          "score": 55.84,
          "source": "https://huggingface.co/mistralai/Magistral-Small-2506"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 16.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 11.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 41.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 74.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 64.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 7.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 51.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 24.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 96.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 71.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 41.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 24.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 4.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 26.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Devstral Medium 1",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/devstral-2507",
      "release_date": "2025-06-10",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.4,
          "source": "https://mistral.ai/news/devstral-2507"
        },
        {
          "name": "Output cost",
          "score": 2,
          "source": "https://mistral.ai/news/devstral-2507"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 4.4,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 18.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 15.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 4.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 70.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 49.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 33.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 29.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 70.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 6.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 4.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 29.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 28.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 9.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 19.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Devstral Small 1.1",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/devstral-2507",
      "release_date": "2025-06-10",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.3,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 23.5724032,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2507"
        },
        {
          "name": "Active parameters",
          "score": 23.5724032,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2507"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 5.4,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 15.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 12.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 29.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 62.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 41.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 25.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 24.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 63.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 0.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 29.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 34.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 17,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 6.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 28.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Devstral Small 1.0",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/devstral",
      "release_date": "2025-05-21",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.3,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 23.5724032,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2507"
        },
        {
          "name": "Active parameters",
          "score": 23.5724032,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2507"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 5.4,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 17.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 12.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 63.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 43.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 25.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 24.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 68.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 6.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 31.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 26.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 6.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 38,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mistral Medium 3",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-medium-3",
      "release_date": "2025-05-07",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.4,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 7.6,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "HumanEval",
          "score": 92.1,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "LiveCodeBench",
          "score": 30.3,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MultiPL-E",
          "score": 81.4,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "Arena-Hard",
          "score": 97.1,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "IFEval",
          "score": 89.4,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MATH",
          "score": 91,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "GPQA Diamond",
          "score": 57.1,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MMLU-Pro",
          "score": 77.2,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "RULER 32K",
          "score": 96,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "RULER 128K",
          "score": 90.2,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MMMU",
          "score": 66.1,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "DocVQA",
          "score": 95.3,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "AI2D",
          "score": 93.7,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "ChartQA",
          "score": 82.6,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 17.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 13.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 30.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 76,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 57.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 40,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 33.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 90.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 44,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 30.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 39.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 28,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 3.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 24.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mistral Small 3.1",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-small-3-1",
      "release_date": "2025-03-17",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.3,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 24.01136128,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "Active parameters",
          "score": 24.01136128,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 6.3,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "MMLU",
          "score": 80.62,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MMLU-Pro",
          "score": 66.76,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MATH",
          "score": 69.3,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "GPQA Main",
          "score": 44.42,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "GPQA Diamond",
          "score": 45.96,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MBPP",
          "score": 74.71,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "HumanEval",
          "score": 88.41,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "SimpleQA",
          "score": 10.43,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MMMU",
          "score": 64,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MMMU-Pro",
          "score": 49.25,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MathVista",
          "score": 68.91,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "ChartQA",
          "score": 86.24,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "DocVQA",
          "score": 94.08,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "AI2D",
          "score": 93.72,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MM MT-Bench",
          "score": 7.3,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "LongBench v2",
          "score": 37.18,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "RULER 32K",
          "score": 93.96,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "RULER 128K",
          "score": 81.2,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 14,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 13.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 3.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 65.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 45.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 21.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 26.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 70.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 9.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 3.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 29.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 19.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 7.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 25.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mistral Small 3",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-small-3",
      "release_date": "2025-01-30",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.3,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 23.5724032,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "Active parameters",
          "score": 23.5724032,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 6.3,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "MMLU-Pro",
          "score": 66.3,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "GPQA",
          "score": 45.3,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "HumanEval",
          "score": 84.8,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "MATH",
          "score": 70.6,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "MT-Bench Dev",
          "score": 8.35,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "WildBench",
          "score": 52.27,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "Arena-Hard",
          "score": 87.3,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "IFEval",
          "score": 82.9,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 12.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 4.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 65.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 46.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 25.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 23.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 71.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 4.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 26.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 19.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Pixtral Large",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/pixtral-large",
      "release_date": "2024-11-18",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 6,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 124,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "Active parameters",
          "score": 124,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 5.4,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "MathVista",
          "score": 69.4,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "MMMU",
          "score": 64,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "ChartQA",
          "score": 88.1,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "DocVQA",
          "score": 93.3,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "VQAv2",
          "score": 80.9,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "AI2D",
          "score": 93.8,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "MM MT-Bench",
          "score": 7.4,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 14,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 2.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 70.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 50.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 26.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 29.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 71.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 2.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 34.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 10.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 36.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Ministral 8B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/ministraux",
      "release_date": "2024-10-16",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://mistral.ai/news/ministraux"
        },
        {
          "name": "Output cost",
          "score": 0.1,
          "source": "https://mistral.ai/news/ministraux"
        },
        {
          "name": "Size",
          "score": 8.019808256,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "Active parameters",
          "score": 8.019808256,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 5.8,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "MT-Bench",
          "score": 8.3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "Arena-Hard",
          "score": 70.9,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "WildBench",
          "score": 41.3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MBPP",
          "score": 70,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "HumanEval",
          "score": 76.8,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MATH",
          "score": 54.5,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU",
          "score": 65,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "AGIEval",
          "score": 48.3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "WinoGrande",
          "score": 75.3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "ARC-Challenge",
          "score": 71.9,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "TriviaQA",
          "score": 65.5,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (fr)",
          "score": 57.5,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (de)",
          "score": 57.4,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (es)",
          "score": 59.6,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        }
      ]
    },
    {
      "name": "Ministral 3B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/ministraux",
      "release_date": "2024-10-16",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.04,
          "source": "https://mistral.ai/news/ministraux"
        },
        {
          "name": "Output cost",
          "score": 0.04,
          "source": "https://mistral.ai/news/ministraux"
        },
        {
          "name": "Size",
          "score": 3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "Active parameters",
          "score": 3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MT-Bench",
          "score": 8.1,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "Arena-Hard",
          "score": 64.3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "WildBench",
          "score": 36.3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MBPP",
          "score": 67.7,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "HumanEval",
          "score": 77.4,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MATH",
          "score": 51.7,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU",
          "score": 60.9,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "AGIEval",
          "score": 42.1,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "WinoGrande",
          "score": 72.7,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "ARC-Challenge",
          "score": 64.2,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "TriviaQA",
          "score": 56.7,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (fr)",
          "score": 49.1,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (de)",
          "score": 48.3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (es)",
          "score": 49.5,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        }
      ]
    },
    {
      "name": "Mistral Small 2 2409",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/september-24-release",
      "release_date": "2024-09-17",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.3,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 22.247282688,
          "source": "https://huggingface.co/mistralai/Mistral-Small-Instruct-2409"
        },
        {
          "name": "Active parameters",
          "score": 22.247282688,
          "source": "https://huggingface.co/mistralai/Mistral-Small-Instruct-2409"
        },
        {
          "name": "GSM8K",
          "score": 88,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "MATH",
          "score": 60,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "HumanEval",
          "score": 78,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "HumanEval Plus",
          "score": 76,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "MT-Bench",
          "score": 8.1,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "WildBench",
          "score": 48,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 10.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 52.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 38.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 14.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 15.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 56.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 6.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Pixtral 12B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/pixtral-12b",
      "release_date": "2024-09-17",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.15,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.15,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 12,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "Active parameters",
          "score": 12,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "MMMU (thinking)",
          "score": 52.5,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "Mathvista (thinking)",
          "score": 58,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "ChartQA (thinking)",
          "score": 81.8,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "DocVQA",
          "score": 90.7,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "VQAv2",
          "score": 78.6,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "MM MT-Bench",
          "score": 6.05,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "MT-Bench (text)",
          "score": 7.68,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "MM IFEval",
          "score": 52.7,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "IFEval (text)",
          "score": 61.3,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "MMLU",
          "score": 69.2,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "MATH",
          "score": 48.1,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "HumanEval",
          "score": 72,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        }
      ]
    },
    {
      "name": "Mistral Large 2",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-large-2407",
      "release_date": "2024-07-24",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 6,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 122.610069504,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "Active parameters",
          "score": 122.610069504,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "MMLU",
          "score": 84,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "MT-Bench",
          "score": 8.63,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "WildBench",
          "score": 56.3,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "Arena-Hard",
          "score": 73.2,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "HumanEval",
          "score": 92,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "HumanEval Plus",
          "score": 87,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "MBPP",
          "score": 80,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "MBPP EvalPlus",
          "score": 69,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "GSM8K",
          "score": 93,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "MATH",
          "score": 70,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "MATH (thinking)",
          "score": 71.5,
          "source": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 13,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 68.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 47.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 26.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 27.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 71.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 9.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 31.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 1.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 33,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mistral Nemo",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-nemo",
      "release_date": "2024-07-18",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.3,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.3,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 12.2477824,
          "source": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
        },
        {
          "name": "Active parameters",
          "score": 12.2477824,
          "source": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
        },
        {
          "name": "HellaSwag",
          "score": 83.5,
          "source": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
        },
        {
          "name": "WinoGrande",
          "score": 76.8,
          "source": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
        },
        {
          "name": "OpenBookQA",
          "score": 60.6,
          "source": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
        },
        {
          "name": "CommonSenseQA",
          "score": 70.4,
          "source": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
        },
        {
          "name": "TruthfulQA",
          "score": 50.3,
          "source": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
        },
        {
          "name": "MMLU",
          "score": 68,
          "source": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
        },
        {
          "name": "TriviaQA",
          "score": 73.8,
          "source": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
        },
        {
          "name": "NaturalQuestions",
          "score": 31.2,
          "source": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
        },
        {
          "name": "MT-Bench",
          "score": 7.84,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "WildBench",
          "score": 42.57,
          "source": "https://mistral.ai/news/mistral-nemo"
        }
      ]
    },
    {
      "name": "Mixtral 8x22B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-large",
      "release_date": "2024-02-26",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 2,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 6,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 140.630071296,
          "source": "https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1"
        },
        {
          "name": "Active parameters",
          "score": 39,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "HumanEval",
          "score": 45.1,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MBPP",
          "score": 71.2,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "GSM8K",
          "score": 78.6,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "GSM8K (thinking)",
          "score": 88.4,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MATH",
          "score": 41.8,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MMLU",
          "score": 77.75,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "HellaSwag",
          "score": 88.5,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "WinoGrande",
          "score": 84.7,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "ARC-Challenge",
          "score": 91.3,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "TriviaQA",
          "score": 82.2,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "NaturalQuestions",
          "score": 40.1,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 9.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 53.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 33.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 14.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 18.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 54.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mistral Large 1",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-large",
      "release_date": "2024-02-26",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 3,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "Output cost",
          "score": 9,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "MMLU",
          "score": 81.2,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "HellaSwag",
          "score": 89.2,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "WinoGrande",
          "score": 86.7,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "ARC-Challenge",
          "score": 94.2,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "TriviaQA",
          "score": 82.7,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "TruthfulQA",
          "score": 50.5,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "HumanEval",
          "score": 45.1,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "MBPP",
          "score": 73.1,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "GSM8K",
          "score": 81,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 9.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 51.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 35.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 17.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 20.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 52.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mistral Small 1 2402",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-large",
      "release_date": "2024-02-26",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 1,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "Output cost",
          "score": 3,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "Size",
          "score": 22,
          "source": "Guessing that Mixtral was upcycled"
        },
        {
          "name": "Active parameters",
          "score": 22,
          "source": "Guessing that Mixtral was upcycled"
        },
        {
          "name": "GSM8K",
          "score": 80,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "MATH",
          "score": 40,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "HumanEval",
          "score": 62,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "HumanEval Plus",
          "score": 60,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "MT-Bench",
          "score": 7.8,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "WildBench",
          "score": 38,
          "source": "https://mistral.ai/news/september-24-release"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 41.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 30.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 11.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 13.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 56.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 0.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mistral Medium 1",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mistral-large",
      "release_date": "2024-02-26",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Size",
          "score": 70,
          "source": "https://huggingface.co/miqudev/miqu-1-70b"
        },
        {
          "name": "Active parameters",
          "score": 70,
          "source": "https://huggingface.co/miqudev/miqu-1-70b"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 49.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 34.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 9.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 11.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 40.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 3.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mixtral 8x7B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/mixtral-of-experts",
      "release_date": "2023-12-11",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.7,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.7,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 46.702792704,
          "source": "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"
        },
        {
          "name": "Active parameters",
          "score": 12.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "BBQ",
          "score": 55.98,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MMLU",
          "score": 70.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HellaSwag",
          "score": 86.7,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 85.8,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "WinoGrande",
          "score": 81.2,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MBPP",
          "score": 60.7,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "GSM8K",
          "score": 58.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MT-Bench",
          "score": 8.3,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "PIQA",
          "score": 83.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Easy",
          "score": 83.1,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 59.7,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "NaturalQuestions",
          "score": 30.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "TriviaQA",
          "score": 71.5,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HumanEval",
          "score": 40.2,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MATH",
          "score": 28.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HumanEval",
          "score": 40.2,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MBPP",
          "score": 60.7,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "GSM8K",
          "score": 58.4,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "GSM8K (thinking)",
          "score": 74.4,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MATH",
          "score": 28.4,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MMLU",
          "score": 70.63,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "HellaSwag",
          "score": 86.6,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "WinoGrande",
          "score": 81.2,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "ARC-Challenge",
          "score": 85.8,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "TriviaQA",
          "score": 78.4,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "NaturalQuestions",
          "score": 36.5,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 7.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 38.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 29.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 6.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 2.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 29.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Mistral 7B",
      "company": "Mistral AI",
      "url": "https://mistral.ai/news/announcing-mistral-7b",
      "release_date": "2023-09-27",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.25,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Output cost",
          "score": 0.25,
          "source": "https://mistral.ai/pricing#api-pricing"
        },
        {
          "name": "Size",
          "score": 8.019808256,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "Active parameters",
          "score": 8.019808256,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU",
          "score": 60.1,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "HellaSwag",
          "score": 81.3,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "WinoGrande",
          "score": 75.3,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "PIQA",
          "score": 83,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "ARC-Easy",
          "score": 80,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "ARC-Challenge",
          "score": 55.5,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "NaturalQuestions",
          "score": 28.8,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "TriviaQA",
          "score": 69.9,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "HumanEval",
          "score": 30.5,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "MBPP",
          "score": 47.5,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "MATH",
          "score": 13.1,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "GSM8K",
          "score": 52.1,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "MT-Bench",
          "score": 6.84,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "MT-Bench",
          "score": 6.7,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "Arena-Hard",
          "score": 44.3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "WildBench",
          "score": 33.1,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MBPP",
          "score": 50.2,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "HumanEval",
          "score": 38.4,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MATH",
          "score": 13.2,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU",
          "score": 62.5,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "AGIEval",
          "score": 42.5,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "WinoGrande",
          "score": 74.2,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "ARC-Challenge",
          "score": 67.9,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "TriviaQA",
          "score": 62.5,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (fr)",
          "score": 50.6,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (de)",
          "score": 49.6,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (es)",
          "score": 51.4,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU",
          "score": 62.5,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "ARC-Challenge",
          "score": 60.5,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "GSM8K",
          "score": 39.6,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "AGIEval",
          "score": 44,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "DROP",
          "score": 63.8,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "BBH",
          "score": 56,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "WinoGrande",
          "score": 78.5,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "HellaSwag",
          "score": 83,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "MATH",
          "score": 12.7,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "ARC-Easy",
          "score": 80.5,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "PIQA",
          "score": 82.2,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "SIQA",
          "score": 47,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "BoolQ",
          "score": 83.2,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "TriviaQA",
          "score": 62.5,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "NaturalQuestions",
          "score": 23.2,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "HumanEval",
          "score": 26.2,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "MBPP",
          "score": 40.2,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "MT-Bench",
          "score": 6.48,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "WildBench",
          "score": 25.55,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "HumanEval",
          "score": 26.22,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MBPP",
          "score": 50.2,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "GSM8K",
          "score": 36.5,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "GSM8K (thinking)",
          "score": 50,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MATH",
          "score": 12.7,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MMLU",
          "score": 62.47,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "HellaSwag",
          "score": 83.1,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "WinoGrande",
          "score": 78,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "ARC-Challenge",
          "score": 77.2,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "TriviaQA",
          "score": 68.8,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "NaturalQuestions",
          "score": 28.1,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "BBQ",
          "score": null,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MMLU",
          "score": 44.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HellaSwag",
          "score": 77.1,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 43.2,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "WinoGrande",
          "score": 69.5,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MBPP",
          "score": 26.1,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "GSM8K",
          "score": 16,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "PIQA",
          "score": 77.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Easy",
          "score": 68.7,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 39,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "NaturalQuestions",
          "score": 17.5,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "TriviaQA",
          "score": 56.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HumanEval",
          "score": 11.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MATH",
          "score": 3.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 7.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 24.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 17.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 4.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 2.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 12.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 19.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    }
  ]
}