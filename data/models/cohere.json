{
  "models": [
    {
      "name": "Command-A",
      "company": "Cohere",
      "url": "https://cohere.com/blog/command-a",
      "release_date": "2025-03-13",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 2.5,
          "source": "https://cohere.com/blog/command-a"
        },
        {
          "name": "Output cost",
          "score": 10,
          "source": "https://cohere.com/blog/command-a"
        },
        {
          "name": "Size",
          "score": 111.057580032,
          "source": "https://huggingface.co/CohereLabs/c4ai-command-a-03-2025"
        },
        {
          "name": "Active parameters",
          "score": 111.057580032,
          "source": "https://huggingface.co/CohereLabs/c4ai-command-a-03-2025"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 7.5,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "HumanEval",
          "score": 82.9,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "LiveCodeBench",
          "score": 26.3,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MultiPL-E",
          "score": 73.1,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "Arena-Hard",
          "score": 95.1,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "IFEval",
          "score": 89.7,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MATH",
          "score": 82,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "GPQA Diamond",
          "score": 46.5,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MMLU-Pro",
          "score": 68.9,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "RULER 32K",
          "score": 95.6,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "RULER 128K",
          "score": 91.2,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 26.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 19.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 13,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 71.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 52.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 28.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 28.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 81.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 9.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 13,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 36.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 18,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 0.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 15.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Command-R+",
      "company": "Cohere",
      "url": "https://cohere.com/blog/command-r-plus-microsoft-azure",
      "release_date": "2024-04-04",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 3,
          "source": "https://cohere.com/blog/command-r"
        },
        {
          "name": "Output cost",
          "score": 15,
          "source": "https://cohere.com/blog/command-r"
        },
        {
          "name": "Size",
          "score": 104,
          "source": "https://huggingface.co/CohereLabs/c4ai-command-r-plus-08-2024"
        },
        {
          "name": "Active parameters",
          "score": 104,
          "source": "https://huggingface.co/CohereLabs/c4ai-command-r-plus-08-2024"
        },
        {
          "name": "GSM8K",
          "score": 70.7,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MMLU",
          "score": 75.7,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "HellaSwag",
          "score": 88.6,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "WinoGrande",
          "score": 85.4,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "TriviaQA",
          "score": 71,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "LMArena Text",
          "score": 1228,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 5.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 43.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 32.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 12.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 11.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 27.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 0.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Command-R",
      "company": "Cohere",
      "url": "https://cohere.com/blog/command-r",
      "release_date": "2024-03-11",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.5,
          "source": "https://cohere.com/blog/command-r"
        },
        {
          "name": "Output cost",
          "score": 1.5,
          "source": "https://cohere.com/blog/command-r"
        },
        {
          "name": "Size",
          "score": 35,
          "source": "https://huggingface.co/CohereLabs/c4ai-command-a-03-2025"
        },
        {
          "name": "Active parameters",
          "score": 35,
          "source": "https://huggingface.co/CohereLabs/c4ai-command-a-03-2025"
        },
        {
          "name": "GSM8K",
          "score": 56.6,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MMLU",
          "score": 68.2,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "HellaSwag",
          "score": 87,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "WinoGrande",
          "score": 81.5,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "TriviaQA",
          "score": 66.5,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "LMArena Text",
          "score": 1229,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 33.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 28.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 4.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 6.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 16.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 0.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    }
  ]
}