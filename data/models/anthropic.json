{
  "models": [
    {
      "name": "Claude Opus 4.6 Thinking",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-opus-4-6",
      "release_date": "2026-02-05",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 5,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 25,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 65,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "Terminal-Bench",
          "score": 65.4,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "SWE-bench Verified",
          "score": 80.8,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "OSWorld",
          "score": 72.7,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "τ²-Bench Retail",
          "score": 91.9,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 99.3,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MCP Atlas",
          "score": 59.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "BrowseComp",
          "score": 84,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 40,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "Humanity's Last Exam (with tools)",
          "score": 53.1,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "Finance Agent",
          "score": 60.7,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "GDPVal-AA Elo",
          "score": 1606,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "ARC-AGI-2",
          "score": 68.8,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "GPQA Diamond",
          "score": 91.3,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MMMU Pro",
          "score": 73.9,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MMMU Pro (with tools)",
          "score": 77.3,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MMLU",
          "score": 91.1,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 53,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 48.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 89.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 36.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 51.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 53.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 70.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 46.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 92.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Opus 4.6",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-opus-4-6",
      "release_date": "2026-02-05",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 5,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 25,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 46.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 47.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 84,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 18.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 45.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 44.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 58.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 48.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 84.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Opus 4.5 Thinking",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-opus-4-5",
      "release_date": "2025-11-24",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 5,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 25,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 59.3,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "τ²-Bench Retail",
          "score": 88.9,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 98.2,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "MCP Atlas",
          "score": 62.3,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "OSWorld",
          "score": 66.3,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "ARC-AGI-2",
          "score": 37.6,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "GPQA Diamond",
          "score": 87,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "MMMU (validation)",
          "score": 80.7,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "MMMLU",
          "score": 90.8,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "SWE-bench Verified",
          "score": 80.9,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "LMArena Text",
          "score": 1470,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "SWE-bench Verified",
          "score": 80.9,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "SWT-bench",
          "score": 80.2,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "SWE-Perf",
          "score": 4.7,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "SWE-Review",
          "score": 16.2,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "OctoCodingbench",
          "score": 36.2,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE",
          "score": 90.7,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE-Web",
          "score": 89.1,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE-Simulation",
          "score": 84,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE-Android",
          "score": 92.2,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE-iOS",
          "score": 90,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE-Backend",
          "score": 98,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "Toolathlon",
          "score": 43.5,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "BrowseComp",
          "score": 37,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "AIME 2025",
          "score": 91,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "MMLU-Pro",
          "score": 90,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "GPQA Diamond",
          "score": 87,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "Humanity's Last Exam (text)",
          "score": 28.4,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "LiveCodeBench",
          "score": 87,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "SciCode",
          "score": 50,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "IFBench",
          "score": 58,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "LCR",
          "score": 74,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 90,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "Multi-SWE-bench",
          "score": 50,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 77.5,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 57.8,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 49.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 47.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 91.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 89.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 86.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 28.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 87.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 49.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 91.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 58,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 74,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 47,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 89.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 30.8,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "Humanity's Last Exam (with tools)",
          "score": 43.2,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "AIME 2025",
          "score": 92.8,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "HMMT Feb 2025",
          "score": 92.9,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "IMO-AnswerBench",
          "score": 78.5,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "GPQA-Diamond",
          "score": 87,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "MMLU-Pro",
          "score": 89.3,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "MMMU-Pro",
          "score": 74,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "CharXiv [RQ]",
          "score": 67.2,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "MathVision",
          "score": 77.1,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "MathVista",
          "score": 80.2,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "ZeroBench",
          "score": 3,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "ZeroBench (with tools)",
          "score": 9,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "OCRBench",
          "score": 86.5,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "OmniDocBench 1.5",
          "score": 87.7,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "InfoVQA",
          "score": 76.9,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "SimpleVQA",
          "score": 69.7,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "WorldVQA",
          "score": 36.8,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "VideoMMMU",
          "score": 84.4,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "MMVU",
          "score": 77.3,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "MotionBench",
          "score": 60.3,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "LongVideoBench",
          "score": 67.2,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "SWE-bench Verified",
          "score": 80.9,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "SWE-bench Pro",
          "score": 55.4,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 77.5,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 59.3,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "PaperBench",
          "score": 72.9,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "CyberGym",
          "score": 50.6,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "SciCode",
          "score": 49.5,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "OJBench",
          "score": 54.6,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "LiveCodeBench",
          "score": 82.2,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "Longbench v2",
          "score": 64.4,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "AA-LCR",
          "score": 71.3,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "BrowseComp",
          "score": 37,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "BrowseComp",
          "score": 57.8,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "WideSearch",
          "score": 76.2,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "DeepSearchQA",
          "score": 76.1,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "FinSearchComp-global",
          "score": 66.2,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "Seal-0",
          "score": 47.7,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "OSWorld-Verified",
          "score": 66.3,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "WebArena",
          "score": 63.4,
          "source": "https://www.kimi.com/blog/kimi-k2-5.html"
        },
        {
          "name": "Terminal-Bench",
          "score": 59.8,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "SWE-bench Verified",
          "score": 80.9,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "OSWorld",
          "score": 66.3,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "τ²-Bench Retail",
          "score": 88.9,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 98.2,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MCP Atlas",
          "score": 62.3,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "BrowseComp",
          "score": 67.8,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 30.8,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "Humanity's Last Exam (with tools)",
          "score": 43.4,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "Finance Agent",
          "score": 55.9,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "GDPVal-AA Elo",
          "score": 1416,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "ARC-AGI-2",
          "score": 37.6,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "GPQA Diamond",
          "score": 87,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MMMU Pro",
          "score": 70.6,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MMMU Pro (with tools)",
          "score": 73.9,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MMLU",
          "score": 90.8,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        }
      ]
    },
    {
      "name": "Claude Opus 4.5",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-opus-4-5",
      "release_date": "2025-11-24",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 5,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 25,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "SWE-bench Verified",
          "score": 80.9,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "LMArena Text",
          "score": 1467,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 43,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 42.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 62.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 88.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 81,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 12.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 73.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 47,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 62.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 43,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 65.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 40.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 86.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Haiku 4.5 Thinking",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-haiku-4-5",
      "release_date": "2025-10-15",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 1,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 5,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 39,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "SWE-bench Verified (with tools)",
          "score": 73.3,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "Terminal-Bench",
          "score": 41,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "τ²-Bench Retail",
          "score": 83.2,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "τ²-Bench Airline",
          "score": 63.6,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 83,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "OSWorld",
          "score": 50.7,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "AIME 2025 (with tools)",
          "score": 96.3,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "AIME 2025",
          "score": 80.7,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "GPQA Diamond",
          "score": 73,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "MMMLU",
          "score": 83,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "MMMU",
          "score": 73.2,
          "source": "https://www.anthropic.com/news/claude-haiku-4-5"
        },
        {
          "name": "GPQA",
          "score": 73,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "SWE-bench Verified",
          "score": 73.3,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 29.8,
          "source": "https://www.tbench.ai/leaderboard/terminal-bench/2.0"
        },
        {
          "name": "LMArena Text",
          "score": 1403,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 37,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 32.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 83.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 76,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 67.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 9.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 61.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 43.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 83.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 54.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 70.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 27.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 54.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Sonnet 4.5 Thinking",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-sonnet-4-5",
      "release_date": "2025-09-29",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 3,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 15,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 42,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "SWE-bench Verified (with tools)",
          "score": 77.2,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "Terminal-Bench",
          "score": 50,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "τ²-Bench Retail",
          "score": 86.2,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "τ²-Bench Airline",
          "score": 70,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 98,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "OSWorld",
          "score": 61.4,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "AIME 2025",
          "score": 87,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "AIME 2025 (with tools)",
          "score": 100,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "GPQA Diamond",
          "score": 83.4,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "MMMLU",
          "score": 89.1,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "MMMU",
          "score": 77.8,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "Finance Agent",
          "score": 55.3,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "SWE-bench Verified",
          "score": 77.2,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Multi-SWE-bench",
          "score": 44.3,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 68,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Terminal-Bench",
          "score": 50,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "ArtifactsBench",
          "score": 61.5,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "BrowseComp",
          "score": 19.6,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "BrowseComp (zh)",
          "score": 40.8,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "GAIA (text)",
          "score": 71.2,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "xbench-DeepSearch",
          "score": 66,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Humanity's Last Exam (with tools)",
          "score": 24.5,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "τ²-Bench",
          "score": 84.7,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "FinSearchComp-global",
          "score": 60.8,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "AgentCompany",
          "score": 41,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "AIME 2025",
          "score": 88,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "MMLU-Pro",
          "score": 88,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "GPQA Diamond",
          "score": 83,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 17.3,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "LiveCodeBench",
          "score": 71,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "SciCode",
          "score": 45,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "IFBench",
          "score": 57,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "LCR",
          "score": 66,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 78,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 33,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Humanity's Last Exam (text)",
          "score": 19.8,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "Humanity's Last Exam (text) (with tools)",
          "score": 32,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "AIME 2025",
          "score": 87,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "AIME 2025 (with tools)",
          "score": 100,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "HMMT Feb 2025",
          "score": 74.6,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "HMMT Feb 2025 (with tools)",
          "score": 88.8,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "IMO-AnswerBench",
          "score": 65.9,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "GPQA Diamond",
          "score": 83.4,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "MMLU-Pro",
          "score": 87.5,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "MMLU-Redux",
          "score": 95.6,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "Longform Writing",
          "score": 79.8,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "HealthBench",
          "score": 44.2,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "BrowseComp",
          "score": 24.1,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "BrowseComp (zh)",
          "score": 42.4,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "Seal-0",
          "score": 53.4,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "FinSearchComp-T3",
          "score": 44,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "FRAMES",
          "score": 85,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "SWE-bench Verified (with tools)",
          "score": 77.2,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "SWE-bench Multilingual (with tools)",
          "score": 68,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "Multi-SWE-bench (with tools)",
          "score": 44.3,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "SciCode",
          "score": 44.7,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "LiveCodeBench",
          "score": 64,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "OJBench",
          "score": 30.4,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "Terminal-Bench",
          "score": 51,
          "source": "https://moonshotai.github.io/Kimi-K2/thinking.html"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 13.7,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "ARC-AGI-2",
          "score": 13.6,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "GPQA Diamond",
          "score": 83.4,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "AIME 2025",
          "score": 87,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "AIME 2025 (with tools)",
          "score": 100,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "MathArena Apex",
          "score": 1.6,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "MMMU-Pro",
          "score": 68,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "ScreenSpot-Pro",
          "score": 36.2,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "CharXiv Reasoning (with tools)",
          "score": 68.5,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "OmniDocBench 1.5",
          "score": 0.145,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "VideoMMMU",
          "score": 77.8,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "LiveCodeBench Pro",
          "score": 1418,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 42.8,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "SWE-bench Verified",
          "score": 77.2,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "τ²-Bench",
          "score": 84.7,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "VendingBench 2",
          "score": 3838.74,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "FACTS Grounding",
          "score": 50.4,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "SimpleQA Verified",
          "score": 29.3,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "MMLU",
          "score": 89.1,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "Global PIQA",
          "score": 90.1,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "MRCR v2: 8-needle 128K",
          "score": 47.1,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"
        },
        {
          "name": "BFCL v4 MultiTurn",
          "score": 68.68,
          "source": "https://x.ai/news/grok-4-1-fast"
        },
        {
          "name": "SWE-bench Verified",
          "score": 77.2,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 50,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "τ²-Bench Retail",
          "score": 86.2,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 98,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "MCP Atlas",
          "score": 43.8,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "OSWorld",
          "score": 61.4,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "ARC-AGI-2",
          "score": 13.6,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "GPQA Diamond",
          "score": 83.4,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "MMMU (validation)",
          "score": 77.8,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "MMMLU",
          "score": 89.1,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "MMLU-Pro",
          "score": 88.2,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "GPQA Diamond",
          "score": 83.4,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 13.7,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "LiveCodeBench (thinking)",
          "score": 64,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "Codeforces",
          "score": 1480,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "AIME 2025",
          "score": 87,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "HMMT Feb 2025",
          "score": 79.2,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "HMMT Nov 2025",
          "score": 81.7,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 42.8,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "SWE-bench Verified",
          "score": 77.2,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "SWE Multilingual",
          "score": 68,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "BrowseComp",
          "score": 24.1,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "BrowseComp (zh)",
          "score": 42.4,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "τ²-Bench",
          "score": 84.7,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "MCP-Universe",
          "score": 46.5,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "MCP-Mark",
          "score": 33.3,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "Tool-Decathlon",
          "score": 38.6,
          "source": "https://api-docs.deepseek.com/news/news251201"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 42.8,
          "source": "https://www.tbench.ai/leaderboard/terminal-bench/2.0"
        },
        {
          "name": "SWE-bench Verified",
          "score": 77.2,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 68,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 42.8,
          "source": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        {
          "name": "MMLU-Pro",
          "score": 88.2,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "GPQA Diamond",
          "score": 83.4,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "Humanity's Last Exam (text)",
          "score": 13.7,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "Humanity's Last Exam (text) (with tools)",
          "score": 32,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "AIME 2025",
          "score": 87,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "HMMT Feb 2025",
          "score": 79.2,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "IMO-AnswerBench",
          "score": 65.8,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "LiveCodeBench",
          "score": 64,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "SWE-bench Verified",
          "score": 77.2,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 68,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 33.3,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 42.8,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "BrowseComp",
          "score": 24.1,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "BrowseComp (zh)",
          "score": 42.4,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "τ²-Bench",
          "score": 87.2,
          "source": "https://z.ai/blog/glm-4.7"
        },
        {
          "name": "SWE-bench Verified",
          "score": 77.2,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "SWT-bench",
          "score": 69.5,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "SWE-Perf",
          "score": 3,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "SWE-Review",
          "score": 10.5,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "OctoCodingbench",
          "score": 22.8,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE",
          "score": 85.2,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE-Web",
          "score": 87.3,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE-Simulation",
          "score": 79.1,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE-Android",
          "score": 87.5,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE-iOS",
          "score": 81.2,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "VIBE-Backend",
          "score": 90.8,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "Toolathlon",
          "score": 38.9,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "BrowseComp",
          "score": 19.6,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "AIME 2025",
          "score": 88,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "MMLU-Pro",
          "score": 88,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "GPQA Diamond",
          "score": 83,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "Humanity's Last Exam (text)",
          "score": 17.3,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "LiveCodeBench",
          "score": 71,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "SciCode",
          "score": 45,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "IFBench",
          "score": 57,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "LCR",
          "score": 66,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 78,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "Multi-SWE-bench",
          "score": 44.3,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 68,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 50,
          "source": "https://www.minimax.io/news/minimax-m21"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 42.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 38.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 88,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 87.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 83.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 17.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 71.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 44.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 88,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 57.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 65.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 35.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 78.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench",
          "score": 51,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "SWE-bench Verified",
          "score": 77.2,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "OSWorld",
          "score": 61.4,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "τ²-Bench Retail",
          "score": 86.2,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 98,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MCP Atlas",
          "score": 43.8,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "BrowseComp",
          "score": 43.9,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 17.7,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "Humanity's Last Exam (with tools)",
          "score": 33.6,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "Finance Agent",
          "score": 54.2,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "GDPVal-AA Elo",
          "score": 1277,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "ARC-AGI-2",
          "score": 13.6,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "GPQA Diamond",
          "score": 83.4,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MMMU Pro",
          "score": 63.4,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MMMU Pro (with tools)",
          "score": 68.9,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        },
        {
          "name": "MMLU",
          "score": 89.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-6"
        }
      ]
    },
    {
      "name": "Claude Sonnet 4.5",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-sonnet-4-5",
      "release_date": "2025-09-29",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 3,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 15,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 7.9,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "GPQA",
          "score": 83.4,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 78.1,
          "source": "https://x.ai/news/grok-4-1-fast"
        },
        {
          "name": "Reka Research Eval (with tools)",
          "score": 41.2,
          "source": "https://x.ai/news/grok-4-1-fast"
        },
        {
          "name": "FRAMES",
          "score": 85,
          "source": "https://x.ai/news/grok-4-1-fast"
        },
        {
          "name": "X Browse",
          "score": 14.6,
          "source": "https://x.ai/news/grok-4-1-fast"
        },
        {
          "name": "LMArena Text",
          "score": 1450,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 37.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 33.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 37,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 86,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 72.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 7.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 59,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 42.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 37,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 42.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 51.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 28.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 70.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Opus 4.1 Thinking",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-opus-4-1",
      "release_date": "2025-08-05",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 15,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 75,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 30,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "SWE-bench Verified (with tools)",
          "score": 74.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "Terminal-Bench",
          "score": 43.3,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "GPQA Diamond",
          "score": 80.9,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "τ-Bench Retail",
          "score": 82.4,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "τ-Bench Airline",
          "score": 56,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "MMMLU",
          "score": 89.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "MMMU",
          "score": 77.1,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "AIME 2025",
          "score": 78,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "SWE-bench Verified (with tools)",
          "score": 74.5,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "Terminal-Bench",
          "score": 46.5,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "τ²-Bench Retail",
          "score": 86.8,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "τ²-Bench Airline",
          "score": 63,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 71.5,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "OSWorld",
          "score": 44.4,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "AIME 2025",
          "score": 78,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "GPQA Diamond",
          "score": 81,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "MMMLU",
          "score": 89.5,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "MMMU",
          "score": 77.1,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "Finance Agent",
          "score": 50.9,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "GPQA",
          "score": 80.9,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "SWE-bench Verified",
          "score": 74.5,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "SWE-bench Verified",
          "score": 74.5,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 46.5,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "τ²-Bench Retail",
          "score": 86.8,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 71.5,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "MCP Atlas",
          "score": 40.9,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "OSWorld",
          "score": 44.4,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "GPQA Diamond",
          "score": 81,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "MMMU (validation)",
          "score": 77.1,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "MMMLU",
          "score": 89.5,
          "source": "https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf"
        },
        {
          "name": "Terminal-Bench 2.0",
          "score": 38,
          "source": "https://www.tbench.ai/leaderboard/terminal-bench/2.0"
        },
        {
          "name": "LMArena Text",
          "score": 1449,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 31.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 36.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 80.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 88,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 80.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 11.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 65.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 40.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 80.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 55.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 66.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 34.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 71.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Opus 4.1",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-opus-4-1",
      "release_date": "2025-08-05",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 15,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 75,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "LMArena Text",
          "score": 1445,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 23.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Opus 4 Thinking",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-4",
      "release_date": "2025-04-22",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 15,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 75,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 27,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "SWE-bench Verified",
          "score": 72.5,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "Terminal-Bench",
          "score": 39.2,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "GPQA Diamond",
          "score": 79.6,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "τ-Bench Retail",
          "score": 81.4,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "τ-Bench Airline",
          "score": 59.6,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "MMMLU",
          "score": 88.8,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "MMMU",
          "score": 76.5,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "AIME 2025",
          "score": 75.5,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "SWE-bench Verified",
          "score": 72.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "Terminal-Bench",
          "score": 39.2,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "GPQA Diamond",
          "score": 79.6,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "τ-Bench Retail",
          "score": 81.4,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "τ-Bench Airline",
          "score": 59.6,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "MMMLU",
          "score": 88.8,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "MMMU",
          "score": 76.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "AIME 2025",
          "score": 75.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "GPQA Diamond",
          "score": 79.6,
          "source": "https://x.ai/news/grok-4"
        },
        {
          "name": "HMMT Feb 2025",
          "score": 58.3,
          "source": "https://x.ai/news/grok-4"
        },
        {
          "name": "HMMT Feb 2025",
          "score": 58.3,
          "source": "https://x.ai/news/grok-4"
        },
        {
          "name": "AIME 2025",
          "score": 75.5,
          "source": "https://x.ai/news/grok-4"
        },
        {
          "name": "ARC-AGI-2",
          "score": 8.6,
          "source": "https://x.ai/news/grok-4"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 10.7,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "GPQA Diamond",
          "score": 79.6,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "AIME 2025",
          "score": 75.5,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "LiveCodeBench",
          "score": 51.1,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "Aider",
          "score": 72,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "SWE-bench Verified",
          "score": 72.5,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "FACTS Grounding",
          "score": 77.7,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "MMMU",
          "score": 76.5,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "MRCR v2: 8-needle 128K",
          "score": 16.1,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "SWE-bench Verified",
          "score": 72.5,
          "source": "https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905"
        },
        {
          "name": "Terminal-Bench",
          "score": 43.2,
          "source": "https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905"
        },
        {
          "name": "GPQA",
          "score": 79.6,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "LMArena Text",
          "score": 1424,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 27.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 34,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 73.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 87.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 79.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 11.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 63.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 39.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 98.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 75.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 73.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 53.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 33.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 31.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 70.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Opus 4",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-4",
      "release_date": "2025-04-22",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 15,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 75,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "LMArena Text",
          "score": 1413,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 22.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 36.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 86,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 70.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 5.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 54.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 40.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 94.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 56.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 36.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 43.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 36,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Sonnet 4 Thinking",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-4",
      "release_date": "2025-04-22",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 3,
          "source": "https://www.alibabacloud.com/help/en/model-studio/models"
        },
        {
          "name": "Output cost",
          "score": 15,
          "source": "https://www.alibabacloud.com/help/en/model-studio/models"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 43,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "SWE-bench Verified (with tools)",
          "score": 72.7,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "Terminal-Bench",
          "score": 33.5,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "GPQA Diamond",
          "score": 75.4,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "τ-Bench Retail",
          "score": 80.5,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "τ-Bench Airline",
          "score": 60,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "MMMLU",
          "score": 86.5,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "MMMU",
          "score": 74.4,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "AIME 2025",
          "score": 70.5,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "SWE-bench Verified",
          "score": 72.7,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "Terminal-Bench",
          "score": 35.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "GPQA Diamond",
          "score": 75.4,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "τ-Bench Retail",
          "score": 80.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "τ-Bench Airline",
          "score": 60,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "MMMLU",
          "score": 86.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "MMMU",
          "score": 74.4,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "AIME 2025",
          "score": 70.5,
          "source": "https://www.anthropic.com/news/claude-opus-4-1"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 7.8,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "GPQA Diamond",
          "score": 75.4,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "AIME 2025",
          "score": 70.5,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "LiveCodeBench",
          "score": 48.9,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "Aider",
          "score": 61.3,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "SWE-bench Verified",
          "score": 72.7,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "FACTS Grounding",
          "score": 79.1,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "MMMU",
          "score": 74.4,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "MRCR v2: 8-needle 128K",
          "score": 39.1,
          "source": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Pro-Model-Card.pdf"
        },
        {
          "name": "SWE-bench Verified (with tools)",
          "score": 72.7,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "Terminal-Bench",
          "score": 36.4,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "τ²-Bench Retail",
          "score": 83.8,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "τ²-Bench Airline",
          "score": 63,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 49.6,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "OSWorld",
          "score": 42.2,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "AIME 2025",
          "score": 70.5,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "GPQA Diamond",
          "score": 76.1,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "MMMLU",
          "score": 86.5,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "MMMU",
          "score": 74.4,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "Finance Agent",
          "score": 44.5,
          "source": "https://www.anthropic.com/news/claude-sonnet-4-5"
        },
        {
          "name": "SWE-bench Verified",
          "score": 72.7,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Multi-SWE-bench",
          "score": 35.7,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 56.9,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Terminal-Bench",
          "score": 36.4,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "ArtifactsBench",
          "score": 57.3,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "BrowseComp",
          "score": 12.2,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "BrowseComp (zh)",
          "score": 29.1,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "GAIA (text)",
          "score": 68.3,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "xbench-DeepSearch",
          "score": 64.6,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Humanity's Last Exam (with tools)",
          "score": 20.3,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "τ²-Bench",
          "score": 65.5,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "FinSearchComp-global",
          "score": 42,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "AgentCompany",
          "score": 37,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "SWE-bench Verified",
          "score": 72.7,
          "source": "https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 53.3,
          "source": "https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905"
        },
        {
          "name": "Multi-SWE-bench",
          "score": 35.7,
          "source": "https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905"
        },
        {
          "name": "Terminal-Bench",
          "score": 36.4,
          "source": "https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905"
        },
        {
          "name": "SWE-Dev",
          "score": 67.1,
          "source": "https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905"
        },
        {
          "name": "AIME 2025",
          "score": 74,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "MMLU-Pro",
          "score": 84,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "GPQA Diamond",
          "score": 78,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 9.6,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "LiveCodeBench",
          "score": 66,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "SciCode",
          "score": 40,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "IFBench",
          "score": 55,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "LCR",
          "score": 65,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "τ²-Bench Telecom",
          "score": 65,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 30,
          "source": "https://huggingface.co/MiniMaxAI/MiniMax-M2"
        },
        {
          "name": "GPQA",
          "score": 75.4,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "Terminal-Bench",
          "score": 35.5,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "SWE-bench Verified",
          "score": 68,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "SWE-bench Live",
          "score": 27.7,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "SWE-bench Multilingual",
          "score": 53.3,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "Multi-SWE-bench mini",
          "score": 24.8,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "Multi-SWE-bench flash",
          "score": 25,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "Aider",
          "score": 56.4,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "Spider2",
          "score": 31.1,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "WebArena",
          "score": 51.1,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "Mind2Web",
          "score": 47.4,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "BFCL v3",
          "score": 73.3,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "τ-Bench Retail",
          "score": 80.5,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "τ-Bench Airline",
          "score": 60,
          "source": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"
        },
        {
          "name": "LMArena Text",
          "score": 1400,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 38.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 34.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 74.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 84.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 77.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 9.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 65.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 40,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 99.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 77.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 74.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 54.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 64.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 31.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 64.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Sonnet 3.7 Thinking",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-3-7-sonnet",
      "release_date": "2025-02-24",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "thinking",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 3,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 15,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 98,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "SWE-bench Verified",
          "score": 62.3,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "Terminal-Bench",
          "score": 35.2,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "GPQA Diamond",
          "score": 78.2,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "τ-Bench Retail",
          "score": 81.2,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "τ-Bench Airline",
          "score": 58.4,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "MMMLU",
          "score": 85.9,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "MMMU",
          "score": 75,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "AIME 2025",
          "score": 54.8,
          "source": "https://www.anthropic.com/news/claude-4"
        },
        {
          "name": "HumanEval",
          "score": 92.1,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "LiveCodeBench",
          "score": 36,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MultiPL-E",
          "score": 83.4,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "Arena-Hard",
          "score": 93.2,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "IFEval",
          "score": 91.8,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MATH",
          "score": 83,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "GPQA Diamond",
          "score": 69.7,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MMLU-Pro",
          "score": 80,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "RULER 32K",
          "score": 95.7,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "RULER 128K",
          "score": 93.8,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MMMU",
          "score": 71.3,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "DocVQA",
          "score": 84.3,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "AI2D",
          "score": 78.8,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "ChartQA",
          "score": 76.3,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "LMArena Text",
          "score": 1389,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 34.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 27.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 56.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 83.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 77.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 10.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 47.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 40.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 94.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 48.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 56.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 48.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 60.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 21.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 54.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude Sonnet 3.7",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-3-7-sonnet",
      "release_date": "2025-02-24",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 3,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "Output cost",
          "score": 15,
          "source": "https://claude.com/pricing#api"
        },
        {
          "name": "ArtificialAnalysis Consumed Tokens (Millions)",
          "score": 7,
          "source": "https://artificialanalysis.ai/"
        },
        {
          "name": "LMArena Text",
          "score": 1372,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 30.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 26.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 21,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 80.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 65.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 39.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 37.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 85,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 22.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 21,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 44,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 48.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 21.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 50,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude 3.5 Sonnet (new)",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/3-5-models-and-computer-use",
      "release_date": "2024-10-22",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 3,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "Output cost",
          "score": 15,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "GPQA Diamond (thinking)",
          "score": 65,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "MMLU-Pro (thinking)",
          "score": 78,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "HumanEval",
          "score": 93.7,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "MATH (thinking)",
          "score": 78.3,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "AIME 2024 (thinking)",
          "score": 16,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "MMMU (thinking)",
          "score": 70.4,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "SWE-bench Verified",
          "score": 49,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "τ-Bench Retail",
          "score": 69.2,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "τ-Bench Airline",
          "score": 46,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "LMArena Text",
          "score": 1373,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 15.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 30.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 77.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 59.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 38.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 36.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 77.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 15.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude 3.5 Haiku",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/3-5-models-and-computer-use",
      "release_date": "2024-10-22",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.8,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "Output cost",
          "score": 4,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "GPQA Diamond (thinking)",
          "score": 41.6,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "MMLU-Pro (thinking)",
          "score": 65,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "HumanEval",
          "score": 88.1,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "MATH (thinking)",
          "score": 69.2,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "AIME 2024 (thinking)",
          "score": 5.3,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "SWE-bench Verified",
          "score": 40.6,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "τ-Bench Retail",
          "score": 51,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "τ-Bench Airline",
          "score": 22.8,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "LMArena Text",
          "score": 1324,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "MMLU",
          "score": 77.6,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MMLU-Pro",
          "score": 65,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MATH",
          "score": 69.2,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "GPQA Main",
          "score": 37.05,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "GPQA Diamond",
          "score": 41.6,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MBPP",
          "score": 85.6,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "HumanEval",
          "score": 88.1,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "SimpleQA",
          "score": 8.02,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MMMU",
          "score": 60.5,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MMMU-Pro",
          "score": 45.03,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MathVista",
          "score": 61.6,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "ChartQA",
          "score": 87.2,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "DocVQA",
          "score": 90,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "AI2D",
          "score": 92.1,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "MM MT-Bench",
          "score": 6.5,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "LongBench v2",
          "score": 35.19,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "RULER 32K",
          "score": 92.6,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "RULER 128K",
          "score": 91.9,
          "source": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 18.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 10.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 63.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 40.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 31.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 27.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 72.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 3.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 42.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 23.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 2.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 24.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude 3.5 Sonnet",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-3-5-sonnet",
      "release_date": "2024-06-21",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 3,
          "source": "https://www.anthropic.com/news/claude-3-5-sonnet"
        },
        {
          "name": "Output cost",
          "score": 15,
          "source": "https://www.anthropic.com/news/claude-3-5-sonnet"
        },
        {
          "name": "GPQA Diamond (thinking)",
          "score": 59.4,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "MMLU-Pro (thinking)",
          "score": 75.1,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "HumanEval",
          "score": 92,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "MATH (thinking)",
          "score": 71.1,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "AIME 2024 (thinking)",
          "score": 9.6,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "MMMU (thinking)",
          "score": 68.3,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "SWE-bench Verified",
          "score": 33.4,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "τ-Bench Retail",
          "score": 62.6,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "τ-Bench Airline",
          "score": 36,
          "source": "https://www.anthropic.com/news/3-5-models-and-computer-use"
        },
        {
          "name": "AIME 2025",
          "score": 16,
          "source": "https://x.ai/news/grok-3"
        },
        {
          "name": "GPQA",
          "score": 65,
          "source": "https://x.ai/news/grok-3"
        },
        {
          "name": "LiveCodeBench",
          "score": 40.2,
          "source": "https://x.ai/news/grok-3"
        },
        {
          "name": "MMLU-Pro",
          "score": 78,
          "source": "https://x.ai/news/grok-3"
        },
        {
          "name": "LOFT (128k)",
          "score": 69.9,
          "source": "https://x.ai/news/grok-3"
        },
        {
          "name": "SimpleQA",
          "score": 28.4,
          "source": "https://x.ai/news/grok-3"
        },
        {
          "name": "MMMU",
          "score": 70.4,
          "source": "https://x.ai/news/grok-3"
        },
        {
          "name": "DROP",
          "score": 87.1,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "HumanEval",
          "score": 92,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "SWE-bench Verified",
          "score": 49,
          "source": "https://llm-stats.com/benchmarks"
        },
        {
          "name": "GPQA",
          "score": 59.6,
          "source": "https://x.ai/news/grok-2"
        },
        {
          "name": "MMLU",
          "score": 88.3,
          "source": "https://x.ai/news/grok-2"
        },
        {
          "name": "MMLU-Pro",
          "score": 76.1,
          "source": "https://x.ai/news/grok-2"
        },
        {
          "name": "MATH",
          "score": 71.1,
          "source": "https://x.ai/news/grok-2"
        },
        {
          "name": "HumanEval",
          "score": 92,
          "source": "https://x.ai/news/grok-2"
        },
        {
          "name": "MMMU",
          "score": 68.3,
          "source": "https://x.ai/news/grok-2"
        },
        {
          "name": "MathVista",
          "score": 67.7,
          "source": "https://x.ai/news/grok-2"
        },
        {
          "name": "DocVQA",
          "score": 95.2,
          "source": "https://x.ai/news/grok-2"
        },
        {
          "name": "MMLU",
          "score": 88.3,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "MMLU-Redux",
          "score": 88.9,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "MMLU-Pro",
          "score": 78,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "DROP",
          "score": 88.3,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "IFEval",
          "score": 86.5,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "GPQA Diamond",
          "score": 65,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "SimpleQA",
          "score": 28.4,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "FRAMES",
          "score": 72.5,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "LongBench v2",
          "score": 41,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "HumanEval-Mul",
          "score": 81.7,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "LiveCodeBench (thinking)",
          "score": 36.3,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "LiveCodeBench",
          "score": 32.8,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "Codeforces percentile",
          "score": 20.3,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "SWE-bench Verified",
          "score": 50.8,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "Aider-Edit",
          "score": 84.2,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "Aider",
          "score": 45.3,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "AIME 2024",
          "score": 16,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "MATH",
          "score": 78.3,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "CNMO 2024",
          "score": 13.1,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "CLUEWSC",
          "score": 85.4,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "Eval Chinese",
          "score": 76.7,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "SimpleQA Chinese",
          "score": 51.3,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "LMArena Text",
          "score": 1343,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "MathVista",
          "score": 67.1,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "MMMU",
          "score": 68.4,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "ChartQA",
          "score": 89.1,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "DocVQA",
          "score": 88.6,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "VQAv2",
          "score": 69.5,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "AI2D",
          "score": 76.9,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "MM MT-Bench",
          "score": 7.3,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "MMMU (thinking)",
          "score": 68,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "Mathvista (thinking)",
          "score": 64.4,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "ChartQA (thinking)",
          "score": 87.6,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "DocVQA",
          "score": 90.3,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "VQAv2",
          "score": 70.7,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 14.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 26,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 75.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 56,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 31.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 69.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 9.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude 3 Opus",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-3-family",
      "release_date": "2024-03-04",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 15,
          "source": "https://www.prompthub.us/models/claude-3-opus"
        },
        {
          "name": "Output cost",
          "score": 75,
          "source": "https://www.prompthub.us/models/claude-3-opus"
        },
        {
          "name": "MMLU",
          "score": 86.8,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "GPQA Diamond",
          "score": 50.4,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "GSM8K",
          "score": 95,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MATH",
          "score": 60.1,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MGSM",
          "score": 90.7,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "HumanEval",
          "score": 84.9,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "DROP",
          "score": 83.1,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "BIG-Bench Hard",
          "score": 86.8,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "ARC-Challenge",
          "score": 96.4,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "HellaSwag",
          "score": 95.4,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MMMU",
          "score": 59.4,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "ANLS",
          "score": 89.3,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MathVista",
          "score": 50.5,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "AI2D",
          "score": 88.1,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "ChartQA",
          "score": 80.8,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "SWE-bench",
          "score": 3.79,
          "source": "https://arxiv.org/abs/2310.06770"
        },
        {
          "name": "LMArena Text",
          "score": 1323,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 12.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 19.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 69.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 48.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 27.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 23.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 64.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 3.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude 3 Sonnet",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-3-family",
      "release_date": "2024-03-04",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 3,
          "source": "https://www.prompthub.us/models/claude-3-sonnet"
        },
        {
          "name": "Output cost",
          "score": 15,
          "source": "https://www.prompthub.us/models/claude-3-sonnet"
        },
        {
          "name": "MMLU",
          "score": 79,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "GPQA Diamond",
          "score": 40.4,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "GSM8K",
          "score": 92.3,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MATH",
          "score": 43.1,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MGSM",
          "score": 83.5,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "HumanEval",
          "score": 73,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "DROP",
          "score": 78.9,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "BIG-Bench Hard",
          "score": 82.9,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "ARC-Challenge",
          "score": 93.2,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "HellaSwag",
          "score": 89,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MMMU",
          "score": 53.1,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "ANLS",
          "score": 89.5,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MathVista",
          "score": 47.9,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "AI2D",
          "score": 88.7,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "ChartQA",
          "score": 81.1,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "LMArena Text",
          "score": 1282,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 10.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 57.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 40,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 3.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 17.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 22.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 41.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 4.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude 3 Haiku",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-3-family",
      "release_date": "2024-03-04",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.25,
          "source": "https://www.prompthub.us/models/claude-3-opus"
        },
        {
          "name": "Output cost",
          "score": 1.25,
          "source": "https://www.prompthub.us/models/claude-3-opus"
        },
        {
          "name": "MMLU",
          "score": 75.2,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "GPQA Diamond",
          "score": 33.3,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "GSM8K",
          "score": 88.9,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MATH",
          "score": 38.9,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MGSM",
          "score": 75.1,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "HumanEval",
          "score": 75.9,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "DROP",
          "score": 78.4,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "BIG-Bench Hard",
          "score": 73.7,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "ARC-Challenge",
          "score": 89.2,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "HellaSwag",
          "score": 85.9,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MMMU",
          "score": 50.2,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "ANLS",
          "score": 88.8,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MathVista",
          "score": 46.4,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "AI2D",
          "score": 86.7,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "ChartQA",
          "score": 81.7,
          "source": "https://www.anthropic.com/news/claude-3-family"
        },
        {
          "name": "MMMU (thinking)",
          "score": 50.4,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "Mathvista (thinking)",
          "score": 44.8,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "ChartQA (thinking)",
          "score": 69.6,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "DocVQA",
          "score": 74.6,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "VQAv2",
          "score": 68.4,
          "source": "https://huggingface.co/mistralai/Pixtral-12B-2409"
        },
        {
          "name": "LMArena Text",
          "score": 1262,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 9.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 15.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 18.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 39.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Claude 2",
      "company": "Anthropic",
      "url": "https://www.anthropic.com/news/claude-2",
      "release_date": "2023-07-11",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 8,
          "source": "https://www.prompthub.us/models/claude-2"
        },
        {
          "name": "Output cost",
          "score": 24,
          "source": "https://www.prompthub.us/models/claude-2"
        },
        {
          "name": "HumanEval",
          "score": 56,
          "source": "https://www.anthropic.com/news/claude-2"
        },
        {
          "name": "GSM8K",
          "score": 88,
          "source": "https://www.anthropic.com/news/claude-2"
        },
        {
          "name": "GSM8K",
          "score": 88,
          "source": "https://x.ai/news/grok"
        },
        {
          "name": "MMLU",
          "score": 75,
          "source": "https://x.ai/news/grok"
        },
        {
          "name": "HumanEval",
          "score": 70,
          "source": "https://x.ai/news/grok"
        },
        {
          "name": "HNHSME 2023",
          "score": 55,
          "source": "https://x.ai/news/grok"
        },
        {
          "name": "GSM8K",
          "score": 80,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "MATH",
          "score": 34.4,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "BIG-Bench Hard",
          "score": 77.7,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "HumanEval",
          "score": 70,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "DROP",
          "score": 82,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "HellaSwag",
          "score": 86.8,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "SWE-bench",
          "score": 1.97,
          "source": "https://arxiv.org/abs/2310.06770"
        },
        {
          "name": "MMLU",
          "score": 78.5,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "ARC-Challenge",
          "score": 91,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "TriviaQA",
          "score": 87.5,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 9.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 12.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 48.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 34.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 17.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 19.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    }
  ]
}