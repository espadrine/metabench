{
  "models": [
    {
      "name": "Llama 4 Maverick",
      "company": "Meta",
      "url": "https://www.llama.com/models/llama-4/",
      "release_date": "2025-04-05",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.27,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.85,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 400,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "Active parameters",
          "score": 17,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "MMMU",
          "score": 73.4,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "MMMU-Pro",
          "score": 59.6,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "MathVista",
          "score": 73.7,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "ChartQA",
          "score": 90,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "DocVQA",
          "score": 94.4,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "LiveCodeBench",
          "score": 43.4,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "MMLU-Pro",
          "score": 80.5,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "GPQA Diamond",
          "score": 69.8,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "MGSM",
          "score": 92.3,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 35.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 26.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 19.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 80.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 67.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 39.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 33.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 88.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 39,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 19.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 43,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 46,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 6.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 17.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "HumanEval",
          "score": 85.4,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "LiveCodeBench",
          "score": 28.7,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MultiPL-E",
          "score": 76.4,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "Arena-Hard",
          "score": 91.8,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "IFEval",
          "score": 88.9,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MATH",
          "score": 90,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "GPQA Diamond",
          "score": 61.1,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MMLU-Pro",
          "score": 80.4,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "RULER 32K",
          "score": 94.8,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "RULER 128K",
          "score": 86.7,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "MMMU",
          "score": 71.8,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "DocVQA",
          "score": 94.1,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "AI2D",
          "score": 84.4,
          "source": "https://mistral.ai/news/mistral-medium-3"
        },
        {
          "name": "ChartQA",
          "score": 90.4,
          "source": "https://mistral.ai/news/mistral-medium-3"
        }
      ]
    },
    {
      "name": "Llama 4 Scout",
      "company": "Meta",
      "url": "https://www.llama.com/models/llama-4/",
      "release_date": "2025-04-05",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.14,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.55,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 109,
          "source": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct"
        },
        {
          "name": "Active parameters",
          "score": 17,
          "source": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct"
        },
        {
          "name": "MMMU",
          "score": 69.4,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "MMMU-Pro",
          "score": 52.2,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "MathVista",
          "score": 70.7,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "ChartQA",
          "score": 88.8,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "DocVQA",
          "score": 94.4,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "LiveCodeBench",
          "score": 32.8,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "MMLU-Pro",
          "score": 74.3,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "GPQA Diamond",
          "score": 57.2,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "MGSM",
          "score": 90.6,
          "source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 28.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 16.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 14,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 75.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 58.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 29.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 17,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 84.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 28.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 14,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 39.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 25.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 1.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 15.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Llama 3.3 Instruct 70B",
      "company": "Meta",
      "url": "https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md",
      "release_date": "2024-12-06",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://www.llama.com/models/llama-3/"
        },
        {
          "name": "Output cost",
          "score": 0.4,
          "source": "https://www.llama.com/models/llama-3/"
        },
        {
          "name": "Size",
          "score": 70.553706496,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "Active parameters",
          "score": 70.553706496,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "MMLU (thinking)",
          "score": 86,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "MMLU-Pro (thinking)",
          "score": 68.9,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "IFEval",
          "score": 92.1,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "GPQA Diamond (thinking)",
          "score": 50.5,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "HumanEval",
          "score": 88.4,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "MBPP EvalPlus",
          "score": 87.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "MATH (thinking)",
          "score": 77,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "BFCL v2",
          "score": 77.3,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "MGSM",
          "score": 91.1,
          "source": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 27.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 19.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 7.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 71.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 49.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 28.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 26,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 77.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 30,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 7.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 47.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 15,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 2.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 26.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 66.6,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "GPQA",
          "score": 53.1,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "HumanEval",
          "score": 85.4,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "MATH",
          "score": 74.3,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "MT-Bench Dev",
          "score": 7.96,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "WildBench",
          "score": 50.04,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "Arena-Hard",
          "score": 84,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        },
        {
          "name": "IFEval",
          "score": 88.35,
          "source": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
        }
      ]
    },
    {
      "name": "Llama 3.2 Instruct 90B Vision",
      "company": "Meta",
      "url": "https://huggingface.co/meta-llama/Llama-3.2-90B-Vision",
      "release_date": "2024-09-25",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.72,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.72,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 88.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.2-90B-Vision"
        },
        {
          "name": "Active parameters",
          "score": 88.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.2-90B-Vision"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 18.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 67.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 43.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 21.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 24,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 62.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MathVista",
          "score": 49.1,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "MMMU",
          "score": 53.7,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "ChartQA",
          "score": 70.8,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "DocVQA",
          "score": 85.7,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "VQAv2",
          "score": 67,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        },
        {
          "name": "MM MT-Bench",
          "score": 5.5,
          "source": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411"
        }
      ]
    },
    {
      "name": "Llama 3.2 Instruct 11B Vision",
      "company": "Meta",
      "url": "https://huggingface.co/meta-llama/Llama-3.2-11B-Vision",
      "release_date": "2024-09-25",
      "capabilities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.16,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.16,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 10.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.2-11B-Vision"
        },
        {
          "name": "Active parameters",
          "score": 10.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.2-11B-Vision"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 15.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 7.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 1.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 46.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 22.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 5.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 11,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 11.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 51.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 9.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 1.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 30.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 11.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 0.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 14.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Llama 3.2 Instruct 3B",
      "company": "Meta",
      "url": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
      "release_date": "2024-09-25",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.06,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.06,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 3.212749824,
          "source": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
        },
        {
          "name": "Active parameters",
          "score": 3.212749824,
          "source": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 11.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 3.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 34.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 25.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 5.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 8.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 5.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 48.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 6.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 3.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 26.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 21.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MT-Bench",
          "score": 7.2,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "Arena-Hard",
          "score": 46,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "WildBench",
          "score": 27.2,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MBPP",
          "score": 64.6,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "HumanEval",
          "score": 61,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MATH",
          "score": 38.4,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU",
          "score": 56.2,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "AGIEval",
          "score": 37.4,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "WinoGrande",
          "score": 59.6,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "ARC-Challenge",
          "score": 43.1,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "TriviaQA",
          "score": 50.7,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (fr)",
          "score": 42.3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (de)",
          "score": 42.2,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (es)",
          "score": 43.1,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        }
      ]
    },
    {
      "name": "Llama 3.2 Instruct 1B",
      "company": "Meta",
      "url": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct",
      "release_date": "2024-09-25",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.05,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.06,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 1.23,
          "source": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"
        },
        {
          "name": "Active parameters",
          "score": 1.23,
          "source": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 8.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 1.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 0,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 20,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 19.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 5.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 1.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 1.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 14,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 0,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 0,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 22.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 0,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 12.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Llama 3.1 Instruct 405B",
      "company": "Meta",
      "url": "https://ai.meta.com/blog/meta-llama-3-1/",
      "release_date": "2024-07-23",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 1,
          "source": "https://www.llama.com/models/llama-3/"
        },
        {
          "name": "Output cost",
          "score": 1.8,
          "source": "https://www.llama.com/models/llama-3/"
        },
        {
          "name": "Size",
          "score": 405.886943232,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "Active parameters",
          "score": 405.886943232,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "MMLU",
          "score": 87.3,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "MMLU-Pro (thinking)",
          "score": 73.3,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "AGIEval English",
          "score": 71.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "CommonSenseQA",
          "score": 85.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "WinoGrande",
          "score": 86.7,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "BIG-Bench Hard (thinking)",
          "score": 85.9,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "ARC-Challenge",
          "score": 96.9,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "TriviaQA-Wiki",
          "score": 91.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "SQuAD",
          "score": 89.3,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "QuAC",
          "score": 53.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "BoolQ",
          "score": 80,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "DROP",
          "score": 84.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "IFEval",
          "score": 88.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "GPQA",
          "score": 50.7,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "HumanEval",
          "score": 89,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "MBPP",
          "score": 88.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "Multipl-E HumanEval",
          "score": 75.2,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "Multipl-E MBPP",
          "score": 65.7,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "GSM8K (thinking)",
          "score": 96.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "MATH (thinking)",
          "score": 73.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "API-Bank",
          "score": 92,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "BFCL",
          "score": 88.5,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "Gorilla",
          "score": 35.3,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "Nexus",
          "score": 58.7,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "MGSM (thinking)",
          "score": 91.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-405B"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 28.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 22.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 73.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 51.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 30.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 29.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 70.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 21.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 39,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 24.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 6.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 19,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU",
          "score": 88.6,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "MMLU-Redux",
          "score": 86.2,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "MMLU-Pro",
          "score": 73.3,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "DROP",
          "score": 88.7,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "IFEval",
          "score": 86,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "GPQA Diamond",
          "score": 51.1,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "SimpleQA",
          "score": 17.1,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "FRAMES",
          "score": 70,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "LongBench v2",
          "score": 36.1,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "HumanEval-Mul",
          "score": 77.2,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "LiveCodeBench (thinking)",
          "score": 28.4,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "LiveCodeBench",
          "score": 30.1,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "Codeforces percentile",
          "score": 25.3,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "SWE-bench Verified",
          "score": 24.5,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "Aider-Edit",
          "score": 63.9,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "Aider",
          "score": 5.8,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "AIME 2024",
          "score": 23.3,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "MATH",
          "score": 73.8,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "CNMO 2024",
          "score": 6.8,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "CLUEWSC",
          "score": 84.7,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "Eval Chinese",
          "score": 61.5,
          "source": "https://api-docs.deepseek.com/news/news1226"
        },
        {
          "name": "SimpleQA Chinese",
          "score": 50.4,
          "source": "https://api-docs.deepseek.com/news/news1226"
        }
      ]
    },
    {
      "name": "Llama 3.1 Instruct 70B",
      "company": "Meta",
      "url": "https://ai.meta.com/blog/meta-llama-3-1/",
      "release_date": "2024-07-23",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.72,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.72,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 70.553706496,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct"
        },
        {
          "name": "Active parameters",
          "score": 70.553706496,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct"
        },
        {
          "name": "MMLU",
          "score": 83.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "MMLU-Pro (thinking)",
          "score": 66.4,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "AGIEval English",
          "score": 64.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "CommonSenseQA",
          "score": 84.1,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "WinoGrande",
          "score": 83.3,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "BIG-Bench Hard (thinking)",
          "score": 81.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "ARC-Challenge",
          "score": 94.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "TriviaQA-Wiki",
          "score": 89.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "SQuAD",
          "score": 81.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "QuAC",
          "score": 51.1,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "BoolQ",
          "score": 79.4,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "DROP",
          "score": 79.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "IFEval",
          "score": 87.5,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "GPQA",
          "score": 46.7,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "HumanEval",
          "score": 80.5,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "MBPP",
          "score": 86,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "Multipl-E HumanEval",
          "score": 65.5,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "Multipl-E MBPP",
          "score": 62,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "GSM8K (thinking)",
          "score": 95.1,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "MATH (thinking)",
          "score": 68,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "API-Bank",
          "score": 90,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "BFCL",
          "score": 84.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "Gorilla",
          "score": 29.7,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "Nexus",
          "score": 56.7,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "MGSM (thinking)",
          "score": 86.9,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-70B"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 22.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 17.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 67.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 40.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 23.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 26.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 64.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 17.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 34.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 6.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 2.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 15.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Llama 3.1 Instruct 8B",
      "company": "Meta",
      "url": "https://ai.meta.com/blog/meta-llama-3-1/",
      "release_date": "2024-07-23",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 8.030261248,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"
        },
        {
          "name": "Active parameters",
          "score": 8.030261248,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"
        },
        {
          "name": "MMLU",
          "score": 69.4,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "MMLU-Pro (thinking)",
          "score": 48.3,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "AGIEval English",
          "score": 47.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "CommonSenseQA",
          "score": 75,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "WinoGrande",
          "score": 60.5,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "BIG-Bench Hard (thinking)",
          "score": 64.2,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "ARC-Challenge",
          "score": 83.4,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "TriviaQA-Wiki",
          "score": 77.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "SQuAD",
          "score": 77,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "QuAC",
          "score": 44.9,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "BoolQ",
          "score": 75,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "DROP",
          "score": 59.5,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "IFEval",
          "score": 80.4,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "GPQA",
          "score": 30.4,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "HumanEval",
          "score": 72.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "MBPP",
          "score": 72.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "Multipl-E HumanEval",
          "score": 50.8,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "Multipl-E MBPP",
          "score": 52.4,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "GSM8K (thinking)",
          "score": 84.5,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "MATH (thinking)",
          "score": 51.9,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "API-Bank",
          "score": 82.6,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "BFCL",
          "score": 76.1,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "Gorilla",
          "score": 8.2,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "Nexus",
          "score": 38.5,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "MGSM (thinking)",
          "score": 68.9,
          "source": "https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 16.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Coding Index",
          "score": 8.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "ArtificialAnalysis Math Index",
          "score": 4.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 47.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 25.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 5.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 11.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 13.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 51.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 7.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2025",
          "score": 4.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "IFBench",
          "score": 28.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LCR",
          "score": 15.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Terminal-Bench-Hard",
          "score": 0.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "τ²-Bench",
          "score": 16.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MT-Bench",
          "score": 7.5,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "Arena-Hard",
          "score": 62.4,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "WildBench",
          "score": 37,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MBPP",
          "score": 69.7,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "HumanEval",
          "score": 67.1,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MATH",
          "score": 49.3,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU",
          "score": 64.7,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "AGIEval",
          "score": 44.4,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "WinoGrande",
          "score": 74.6,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "ARC-Challenge",
          "score": 46,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "TriviaQA",
          "score": 60.2,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (fr)",
          "score": 50.8,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (de)",
          "score": 52.8,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU (es)",
          "score": 54.6,
          "source": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
        },
        {
          "name": "MMLU",
          "score": 66.6,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "ARC-Challenge",
          "score": 59.2,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "GSM8K",
          "score": 45.7,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "AGIEval",
          "score": 45.9,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "DROP",
          "score": 58.4,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "BBH",
          "score": 61.1,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "WinoGrande",
          "score": 76.1,
          "source": "https://arxiv.org/abs/2408.00118"
        },
        {
          "name": "HellaSwag",
          "score": 82,
          "source": "https://arxiv.org/abs/2408.00118"
        }
      ]
    },
    {
      "name": "Llama 3 Instruct 70B",
      "company": "Meta",
      "url": "https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct",
      "release_date": "2024-04-18",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.65,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.88,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 70.553706496,
          "source": "https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct"
        },
        {
          "name": "Active parameters",
          "score": 70.553706496,
          "source": "https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 13,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 57.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 37.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 19.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 18.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 48.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 0,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Llama 3 Instruct 8B",
      "company": "Meta",
      "url": "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
      "release_date": "2024-04-18",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text",
          "tool"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.05,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.16,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 8.030261248,
          "source": "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
        },
        {
          "name": "Active parameters",
          "score": 8.030261248,
          "source": "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 40.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 29.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 5.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 9.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 11.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 49.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 0,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "HellaSwag",
          "score": 80.6,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "WinoGrande",
          "score": 73.5,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "NaturalQuestions",
          "score": 28.2,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "TriviaQA",
          "score": 61,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "MMLU",
          "score": 62.3,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "OpenBookQA",
          "score": 56.4,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "CommonSenseQA",
          "score": 66.7,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "TruthfulQA",
          "score": 43,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "MT-Bench",
          "score": 6.85,
          "source": "https://mistral.ai/news/mistral-nemo"
        },
        {
          "name": "WildBench",
          "score": 28.77,
          "source": "https://mistral.ai/news/mistral-nemo"
        }
      ]
    },
    {
      "name": "Llama 2 70B Chat",
      "company": "Meta",
      "url": "https://www.llama.com/llama2/",
      "release_date": "2023-07-18",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Size",
          "score": 68.976658432,
          "source": "https://huggingface.co/meta-llama/Llama-2-70b-chat-hf"
        },
        {
          "name": "Active parameters",
          "score": 68.976658432,
          "source": "https://huggingface.co/meta-llama/Llama-2-70b-chat-hf"
        },
        {
          "name": "MMLU",
          "score": 68,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "GSM8K",
          "score": 56.8,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "MATH",
          "score": 13.5,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "BIG-Bench Hard",
          "score": 51.2,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "HumanEval",
          "score": 29.9,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "HellaSwag",
          "score": 80,
          "source": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"
        },
        {
          "name": "GPQA Diamond",
          "score": 26.5,
          "source": "https://arxiv.org/abs/2311.12022"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 5.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 40.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 32.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 9.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 32.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 0,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU",
          "score": 69.9,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "HellaSwag",
          "score": 87.1,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "WinoGrande",
          "score": 83.2,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "ARC-Challenge",
          "score": 86,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "HumanEval",
          "score": 29.3,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "MBPP",
          "score": 49.8,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "GSM8K",
          "score": 53.6,
          "source": "https://mistral.ai/news/mistral-large"
        },
        {
          "name": "HumanEval",
          "score": 29.3,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MBPP",
          "score": 49.8,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "GSM8K",
          "score": 53.6,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "GSM8K (thinking)",
          "score": 69.6,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MATH",
          "score": 13.8,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "MMLU",
          "score": 69.9,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "HellaSwag",
          "score": 87.1,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "WinoGrande",
          "score": 83.2,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "ARC-Challenge",
          "score": 86,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "TriviaQA",
          "score": 77.57,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "NaturalQuestions",
          "score": 35.5,
          "source": "https://mistral.ai/news/mixtral-8x22b"
        },
        {
          "name": "BBQ",
          "score": 51.5,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MMLU",
          "score": 69.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HellaSwag",
          "score": 87.1,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 56.5,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "WinoGrande",
          "score": 80.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MBPP",
          "score": 49.8,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "GSM8K",
          "score": 53.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MT-Bench",
          "score": 6.86,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "PIQA",
          "score": 82.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Easy",
          "score": 79.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 56.5,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "NaturalQuestions",
          "score": 25.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "TriviaQA",
          "score": 73,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HumanEval",
          "score": 29.3,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MATH",
          "score": 13.8,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "LMArena Text",
          "score": 1172,
          "source": "https://lmarena.ai/"
        }
      ]
    },
    {
      "name": "Llama 2 13B Chat",
      "company": "Meta",
      "url": "https://www.llama.com/llama2/",
      "release_date": "2023-07-18",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Size",
          "score": 13.01586944,
          "source": "https://huggingface.co/meta-llama/Llama-2-13b-chat-hf"
        },
        {
          "name": "Active parameters",
          "score": 13.01586944,
          "source": "https://huggingface.co/meta-llama/Llama-2-13b-chat-hf"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 5.5,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 40.6,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 32.1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 4.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 9.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 11.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 32.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 1.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU",
          "score": 55.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HellaSwag",
          "score": 80.7,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 48.8,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "WinoGrande",
          "score": 72.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MBPP",
          "score": 35.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "GSM8K",
          "score": 6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "PIQA",
          "score": 80.8,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Easy",
          "score": 75.2,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 48.8,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "NaturalQuestions",
          "score": 16.7,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "TriviaQA",
          "score": 64,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HumanEval",
          "score": 18.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MATH",
          "score": 8.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MMLU",
          "score": 55.6,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "HellaSwag",
          "score": 80.7,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "WinoGrande",
          "score": 72.9,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "PIQA",
          "score": 80.8,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "ARC-Easy",
          "score": 75.2,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "ARC-Challenge",
          "score": 48.8,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "NaturalQuestions",
          "score": 29,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "TriviaQA",
          "score": 69.6,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "HumanEval",
          "score": 18.9,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "MBPP",
          "score": 35.4,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "MATH",
          "score": 6,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "GSM8K",
          "score": 34.3,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "LMArena Text",
          "score": 1143,
          "source": "https://lmarena.ai/"
        }
      ]
    },
    {
      "name": "Llama 2 7B Chat",
      "company": "Meta",
      "url": "https://www.llama.com/llama2/",
      "release_date": "2023-07-18",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Input cost",
          "score": 0.05,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Output cost",
          "score": 0.25,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Size",
          "score": 6.738417664,
          "source": "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf"
        },
        {
          "name": "Active parameters",
          "score": 6.738417664,
          "source": "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 11.3,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU-Pro",
          "score": 16.4,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "GPQA Diamond",
          "score": 22.7,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "Humanity's Last Exam",
          "score": 5.8,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "LiveCodeBench",
          "score": 0.2,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "SciCode",
          "score": 0,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MATH",
          "score": 5.9,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "AIME 2024",
          "score": 0,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        },
        {
          "name": "MMLU",
          "score": 44.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HellaSwag",
          "score": 77.1,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 43.2,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "WinoGrande",
          "score": 69.5,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MBPP",
          "score": 26.1,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "GSM8K",
          "score": 3.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "PIQA",
          "score": 77.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Easy",
          "score": 68.7,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 39,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "NaturalQuestions",
          "score": 17.5,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "TriviaQA",
          "score": 56.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HumanEval",
          "score": 11.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MATH",
          "score": 3.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MMLU",
          "score": 44.4,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "HellaSwag",
          "score": 77.1,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "WinoGrande",
          "score": 69.5,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "PIQA",
          "score": 77.9,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "ARC-Easy",
          "score": 68.7,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "ARC-Challenge",
          "score": 43.2,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "NaturalQuestions",
          "score": 24.7,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "TriviaQA",
          "score": 63.8,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "HumanEval",
          "score": 11.6,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "MBPP",
          "score": 26.1,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "MATH",
          "score": 3.9,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "GSM8K",
          "score": 16,
          "source": "https://mistral.ai/news/announcing-mistral-7b"
        },
        {
          "name": "LMArena Text",
          "score": 1109,
          "source": "https://lmarena.ai/"
        }
      ]
    },
    {
      "name": "Llama 65B",
      "company": "Meta",
      "url": "https://arxiv.org/abs/2302.13971",
      "release_date": "2023-02-24",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Size",
          "score": 65.2,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "Active parameters",
          "score": 65.2,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "BoolQ",
          "score": 85.3,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "PIQA",
          "score": 82.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "SIQA",
          "score": 52.3,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "HellaSwag",
          "score": 84.2,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "WinoGrande",
          "score": 77,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "ARC-Easy",
          "score": 78.9,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "ARC-Challenge",
          "score": 56,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "OpenBookQA",
          "score": 60.2,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "NaturalQuestions",
          "score": 23.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "TriviaQA",
          "score": 68.2,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "RACE-middle",
          "score": 67.9,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "RACE-high",
          "score": 51.6,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MATH",
          "score": 10.6,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "GSM8k",
          "score": 50.9,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "HumanEval",
          "score": 23.7,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MBPP",
          "score": 37.7,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MMLU",
          "score": 63.4,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 1,
          "source": "https://artificialanalysis.ai/api/v2/data/llms/models"
        }
      ]
    },
    {
      "name": "Llama 33B",
      "company": "Meta",
      "url": "https://arxiv.org/abs/2302.13971",
      "release_date": "2023-02-24",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Size",
          "score": 32.5,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "Active parameters",
          "score": 32.5,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "BoolQ",
          "score": 83.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "PIQA",
          "score": 82.3,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "SIQA",
          "score": 50.4,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "HellaSwag",
          "score": 82.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "WinoGrande",
          "score": 76,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "ARC-Easy",
          "score": 80,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "ARC-Challenge",
          "score": 57.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "OpenBookQA",
          "score": 58.6,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "NaturalQuestions",
          "score": 24.9,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "TriviaQA",
          "score": 65.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "RACE-middle",
          "score": 64.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "RACE-high",
          "score": 48.3,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MATH",
          "score": 7.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "GSM8k",
          "score": 35.6,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "HumanEval",
          "score": 21.7,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MBPP",
          "score": 30.2,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MMLU",
          "score": 57.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MMLU",
          "score": 56.8,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HellaSwag",
          "score": 83.7,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 54.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "WinoGrande",
          "score": 76.2,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MBPP",
          "score": 40.9,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "GSM8K",
          "score": 8.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "PIQA",
          "score": 82.2,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Easy",
          "score": 79.6,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ARC-Challenge",
          "score": 54.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "NaturalQuestions",
          "score": 24.1,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "TriviaQA",
          "score": 68.5,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "HumanEval",
          "score": 25,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "MATH",
          "score": 8.4,
          "source": "https://mistral.ai/news/mixtral-of-experts"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 0,
          "source": "Educated guess"
        }
      ]
    },
    {
      "name": "Llama 13B",
      "company": "Meta",
      "url": "https://arxiv.org/abs/2302.13971",
      "release_date": "2023-02-24",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Size",
          "score": 13,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "Active parameters",
          "score": 13,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "BoolQ",
          "score": 78.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "PIQA",
          "score": 80.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "SIQA",
          "score": 50.4,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "HellaSwag",
          "score": 79.2,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "WinoGrande",
          "score": 73,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "ARC-Easy",
          "score": 74.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "ARC-Challenge",
          "score": 52.7,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "OpenBookQA",
          "score": 56.4,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "NaturalQuestions",
          "score": 20.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "TriviaQA",
          "score": 56.6,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "RACE-middle",
          "score": 61.6,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "RACE-high",
          "score": 47.2,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MATH",
          "score": 3.9,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "GSM8k",
          "score": 17.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "HumanEval",
          "score": 15.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MBPP",
          "score": 22,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MMLU",
          "score": 46.9,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "LMArena Text",
          "score": 972,
          "source": "https://lmarena.ai/"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 0,
          "source": "Educated guess"
        }
      ]
    },
    {
      "name": "Llama 7B",
      "company": "Meta",
      "url": "https://arxiv.org/abs/2302.13971",
      "release_date": "2023-02-24",
      "capabilities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "benchmarks": [
        {
          "name": "Size",
          "score": 6.7,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "Active parameters",
          "score": 6.7,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "BoolQ",
          "score": 76.5,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "PIQA",
          "score": 79.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "SIQA",
          "score": 48.9,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "HellaSwag",
          "score": 76.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "WinoGrande",
          "score": 70.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "ARC-Easy",
          "score": 72.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "ARC-Challenge",
          "score": 47.6,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "OpenBookQA",
          "score": 57.2,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "NaturalQuestions",
          "score": 16.8,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "TriviaQA",
          "score": 50,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "RACE-middle",
          "score": 61.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "RACE-high",
          "score": 46.9,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MATH",
          "score": 2.9,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "GSM8k",
          "score": 11,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "HumanEval",
          "score": 10.5,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MBPP",
          "score": 17.7,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "MMLU",
          "score": 35.1,
          "source": "https://arxiv.org/abs/2302.13971"
        },
        {
          "name": "ArtificialAnalysis Intelligence Index",
          "score": 0,
          "source": "Educated guess"
        }
      ]
    }
  ]
}